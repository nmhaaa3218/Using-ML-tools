{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc04cb78",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Sign Language Image Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de859f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this assignment you will implement different deep learning networks to classify images of hands in poses that correspond to letters in American Sign Language. The dataset is contained in the assignment zip file, along with some images and a text file describing the dataset. It is similar in many ways to other MNIST datasets.\n",
    "\n",
    "The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (**excluding J and Z which require motion**). The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and **no cases for 9=J or 25=Z because of gesture motions**). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, $pixel_{1}$,$pixel_{2}$â€¦.$pixel_{784}$ which represent a single 28x28 pixel image with grayscale values between 0-255.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "A client is interested in having you (or rather the company that you work for) investigate whether it is possible to develop an app that would enable American sign language to be translated for people that do not sign, or those that sign in different languages/styles. They have provided you with a labelled data of images related to signs (hand positions) that represent individual letters in order to do a preliminary test of feasibility.\n",
    "\n",
    "Your manager has asked you to do this feasibility assessment, but subject to a constraint on the computational facilities available.  More specifically, you are asked to do **no more than 100 training runs in total** (including all models and hyperparameter settings that you consider).  \n",
    "\n",
    "The task requires you to create a Jupyter Notebook to perform 22 steps. These steps involve loading the dataset, fixing data problems, converting labels to one-hot encoding, plotting sample images, creating, training, and evaluating two sequential models with 20 Dense layers with 100 neurons each, checking for better accuracy using MC Dropout, retraining the first model with performance scheduling, evaluating both models, using transfer learning to create a new model using pre-trained weights, freezing the weights of the pre-trained layers, adding new Dense layers, training and evaluating the new model, predicting and converting sign language to text using the best model.\n",
    "\n",
    "### IMPORTANT\n",
    "* Train all the models locally on your own machine. No model training should occur on Gradescope (GS).\n",
    "* After completing the training, upload the trained models' **h5 files** and their training histories along with your notebook to GS.\n",
    "    * best_dnn_bn_model.keras\n",
    "    * best_dnn_bn_perf_model.keras\n",
    "    * best_dnn_selu_model.keras\n",
    "    * best_mobilenet_model.keras\n",
    "    * history1\n",
    "    * history2\n",
    "    * history1_perf\n",
    "    * historymb\n",
    "* To avoid any confusion and poor training on GS, please remember to comment out the training code in your notebook before uploading it to GS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1095cc67-6b26-4695-a4cd-5d1996cf9290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Install opencv using \"pip install opencv-python\" in order to use cv2.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries (TensorFlow, sklearn NumPy, Pandas, and Matplotlib)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Install opencv using \"pip install opencv-python\" in order to use cv2.\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a681f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step0** This test is for checking whether all the required files are submitted. Once you submit all the required files to the autograder, you will be able to pass this step.\n",
    "\n",
    "**IMPORTANT:** Run this step to determine whether you have created all the required files correctly.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c363a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Don't change this cell code\n",
    "required_files = ['best_dnn_bn_model.keras','best_dnn_bn_perf_model.keras','best_dnn_selu_model.keras','best_mobilenet_model.keras','history1','history2','history1_perf','historymb']\n",
    "step0_files = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file) == False:\n",
    "        step0_files = False\n",
    "        print(\"One or more files are missing!\")\n",
    "        break\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.   \n",
    "step0_data = step0_files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51bd8fdb-6300-46fa-81a7-e1c5de1e6377",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5069b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35b67c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP1** Load the dataset (train and test) using `Pandas` from the CSV file.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1538915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset using Pandas from the CSV file\n",
    "train_df = ...\n",
    "test_df = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step1_sol = test_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e64db119-ec76-4834-9ed8-74d213781695",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f455b8d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP2** Examine the data and fix any problems. It is important that you don't have gaps in the number of classes, therefore, check the classes which are not available and shift the labels in order to ensure 24 classes, starting from class 0. In addition, normalize the values of your images in a range of 0 and 1.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390567e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate labels and pixel values in training and testing sets\n",
    "train_labels = ...\n",
    "test_labels = ...\n",
    "train_images = ...\n",
    "test_images = ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step2_sol = {\n",
    "    \"max_label_train\": max(train_labels),\n",
    "    \"min_label_train\": min(train_labels),\n",
    "    \"max_pixel_value_train\": max(train_images.ravel()),\n",
    "    \"min_pixel_value_train\": min(train_images.ravel()),\n",
    "    \"max_label_test\": max(test_labels),\n",
    "    \"min_label_test\": min(test_labels),\n",
    "    \"max_pixel_value_test\": max(test_images.ravel()),\n",
    "    \"min_pixel_value_test\": min(test_images.ravel())\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28e6ed83-eeec-4308-af55-921a6572c28b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e153719",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP3** Convert Labels to One-Hot Encoding both train and test.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3bda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels_encoded = ...\n",
    "test_labels_encoded = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step3_sol = {\n",
    "    \"train_labels_shape\": train_labels_encoded.shape,\n",
    "    \"test_labels_shape\": test_labels_encoded.shape,\n",
    "    \"sample_train_label\": train_labels_encoded[0],\n",
    "    \"sample_test_label\": test_labels_encoded[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b284a53-ebf6-433f-a027-b71f334fdb19",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c368ad",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP4** Plot one sample image for each letter in the dataset given in the training set. To solve this step you should use the function `imshow` to diplay your images in a similar to the image below.\n",
    "\n",
    "<center><img src=\"example_letters.jpg\" width=400 height=300/></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28128d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get one sample image for each label\n",
    "\n",
    "# Plot the sample images"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5734168d-5652-4e1d-be0a-d6b6970cf2ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc18e0e",
   "metadata": {},
   "source": [
    "## Create Neural Network Architectures\n",
    "\n",
    "In this part you should create two different models (model1 and model2) with different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809ebf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP5** \n",
    "Create one sequential model in `TensorFlow` with 20 Dense layers with 100 neurons each one. Consider the specific modifications that you need to do in order to work with your specific input and those to get the required output. \n",
    "\n",
    "* This model uses Batch Normalization after each Dense layer and uses He initialization for all of them. Apply Swish activation following each Batch Normalization.\n",
    "* The input layer should be able to accept data of shape (784,), which corresponds to the flattened image data. The output layer should produce 24 outputs to correspond to each class.\n",
    "\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0df7dd-4883-4b8a-ab2e-70ddbbdf19d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "# Model 1: with Batch Normalization\n",
    "model1 = ...\n",
    "    ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step5_sol = {\n",
    "    \"total_layers\": len(model1.layers),\n",
    "    \"output_activation\": model1.layers[-1].activation.__name__,\n",
    "    \"first_layer_input_shape\": model1.layers[0].input.shape,\n",
    "    \"last_layer_output_units\": model1.layers[-1].units,\n",
    "    \"batch_norm_count\": sum([1 for layer in model1.layers if isinstance(layer, BatchNormalization)]),\n",
    "    \"swish_activation_count\": sum([1 for layer in model1.layers if hasattr(layer, 'activation') and layer.activation == swish]),\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20978bad-6613-46bd-9925-c3835663c1b8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed321f6a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP6** \n",
    "Create a second sequential model in `TensorFlow` . This model should incorporate specific features to promote self-normalization and include regularization:\n",
    "\n",
    "* Layers and Neurons: Construct the model with 20 Dense layers, each containing 100 neurons.\n",
    "* Activation and Initialization:\n",
    "  * Use SELU (Scaled Exponential Linear Unit) activation for each Dense layer to ensure self-normalization.\n",
    "  * Initialize weights using the LeCun normal initializer in all Dense layers.\n",
    "* Regularization:\n",
    "  * Integrate Dropout with a rate of 0.1 after every second Dense layer starting from the second Dense layer to enhance model regularization without breaking self-normalization.\n",
    "* Input and Output:\n",
    "  * The input layer should accept flattened images with a dimension of 784 (28x28 images flattened).\n",
    "  * The output layer should consist of 24 neurons with softmax activation to handle multi-class classification.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe3f51-f643-4cb8-916e-d136cfb88bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Model 2: with SELU and self-normalization\n",
    "model2 = ...\n",
    "    ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step6_sol = {\n",
    "    \"total_layers\": len(model2.layers),\n",
    "    \"dropout_count\": sum([1 for layer in model2.layers if isinstance(layer, Dropout)]),\n",
    "    \"selu_count\": sum([1 for layer in model2.layers if hasattr(layer, 'activation') and layer.activation.__name__ == 'selu']),\n",
    "    \"first_layer_config\": model2.layers[0].get_config(),\n",
    "    \"output_layer_activation\": model2.layers[-1].activation.__name__\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27b1e826-ab57-41df-baf3-c8e8a10c3709",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3173843",
   "metadata": {},
   "source": [
    "## Compile the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94407f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP7** \n",
    "Compile the both previous models using **Nadam** optimization. Also:\n",
    "\n",
    "* Set the loss function to categorical cross-entropy. \n",
    "* Set the metric to accuracy.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2176c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile first model\n",
    "\n",
    "# Compile second model\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step7_sol = {\n",
    "    \"model1_config\": {\n",
    "        \"optimizer\": str(model1.optimizer.__class__.__name__),\n",
    "        \"loss\": model1.loss        \n",
    "    },\n",
    "    \"model2_config\": {\n",
    "        \"optimizer\": str(model2.optimizer.__class__.__name__),\n",
    "        \"loss\": model2.loss,        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce05a077-d493-4ae4-a55b-275a5dc87793",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11a550",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709e2ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP8** One of these models work preferably with data, which follow a normal distribution. Generate **train_images_scaled** and **test_images_scaled** using a `Sklearn` function that allow you to convert data to a normal distribution with mean 0 and variance equal 1. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062385a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_images_scaled = ...\n",
    "test_images_scaled = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step8_sol = (np.mean(train_images_scaled),np.mean(test_images_scaled),np.std(train_images_scaled),np.std(test_images_scaled))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60c26992-cd9d-40b9-a5fe-7f52d9832020",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bce9ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP9** Train the two models on the training dataset using early stopping. In order to save the results given by your training for your models, create checkpoints saving the best model in each case using the function `ModelCheckpoint`. Note that one of the models use the scaled data obtained in **STEP8**. Meanwhile, the other model does not. Figure out which is the proper input data for each model.\n",
    "\n",
    "* Limit the number of epochs to 100. Set the batch size to or greater than 32.\n",
    "\n",
    "**IMPORTANT:** Comment out the code to train/fit the two models. Keep the code given to save the models.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e2814-3763-45a5-afff-8da2f4511ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "...\n",
    "...\n",
    "\n",
    "# The following line of code is used by the autograder. Do not modify it.\n",
    "history1, history2 = None, None\n",
    " \n",
    "# We go to set a seed to get the same results every run\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = ...\n",
    "# Define model checkpoint callback\n",
    "model1_checkpoint_cb = ...\n",
    "model2_checkpoint_cb = ...\n",
    "\n",
    "#################################### Perform the training on your machine and then comment out the following section before uploading it to gradescope. \n",
    "# Make sure your best model is named as follows:\n",
    "# Model1 filename = best_dnn_bn_model.keras\n",
    "# Model2 filename = best_dnn_selu_model.keras\n",
    "\n",
    "# Train model 1 (Comment this out before submission)\n",
    "history1 = ...\n",
    "\n",
    "# Train model 2 (Comment this out before submission)\n",
    "history2 = ...\n",
    "\n",
    "# The following code will save your history - don't change it.\n",
    "if 'history1' in globals():  \n",
    "    with open('./history1', 'wb') as file_pi:\n",
    "        pickle.dump(history1.history, file_pi)\n",
    "if 'history2' in globals():  \n",
    "    with open('./history2', 'wb') as file_pi:\n",
    "        pickle.dump(history2.history, file_pi)\n",
    "####################################\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step9_sol = (model1_checkpoint_cb, model2_checkpoint_cb, early_stopping)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24c47028-ebc2-4a5b-ba3f-3fa52d35a07f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf9ee5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP10** After training, you must now evaluate the performance of the two previously saved models using the test datasets. Each model has different data requirements:\n",
    "* Model 1 should be evaluated using the original test dataset.\n",
    "* Model 2 should be evaluated using the scaled test dataset.\n",
    "Load each model from their saved states, then perform the evaluation. To pass this step, both models must achieve a test accuracy greater than 50%.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47883fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change the following 4 lines of code.\n",
    "# define the file name for the saved model\n",
    "model1_name = \"best_dnn_bn_model.keras\"\n",
    "# load the model\n",
    "model1 = keras.models.load_model(model1_name)\n",
    "# define the file name for the saved model\n",
    "model2_name = \"best_dnn_selu_model.keras\"\n",
    "# load the model\n",
    "model2 = keras.models.load_model(model2_name)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "test_loss1, test_acc1 = ...\n",
    "test_loss2, test_acc2 = ...\n",
    "print(f\"Model 1| Test accuracy: {test_acc1:.4f}, Test loss: {test_loss1:.4f}\")\n",
    "print(f\"Model 2| Test accuracy: {test_acc2:.4f}, Test loss: {test_loss2:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step10_sol = (test_loss1, test_acc1, test_loss2, test_acc2, model1, model2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63e71c8f-880b-4998-8030-b00294774dd7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195fb8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP11** From the loaded history of the two trained models, plot a graph of **accuracy** vs **number of epochs** for both training and validation. \n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8d535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load history for model 1 (Do not modify this code)\n",
    "history_name1 = \"./history1\"\n",
    "with open(history_name1, \"rb\") as file_pi:\n",
    "    loaded_history1 = pickle.load(file_pi)\n",
    "    \n",
    "# Load history for model 2 (Do not modify this code)\n",
    "history_name2 = \"./history2\"\n",
    "with open(history_name2, \"rb\") as file_pi:\n",
    "    loaded_history2 = pickle.load(file_pi)\n",
    "\n",
    "# Plot the training and validation accuracies during training for both models\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step11_sol = (loaded_history1, loaded_history2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c904659-765a-4fe6-888e-b7b15489e5d9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed0f51",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5094c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP12** Check if model2 achieves better accuracy using MC Dropout (without retraining).\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function computes the MC (Monte Carlo) Dropout predictions for a given model and input data. \n",
    "# It returns the mean of multiple predictions obtained by running the model in training mode.\n",
    "#Parameters\n",
    "#   model: A trained model with a dropout layers.\n",
    "#   X    : The input data for which the predictions are to be made.\n",
    "#   n_samples: The number of Monte Carlo samples to generate..\n",
    "#Returns\n",
    "  # The function returns an array-like object containing the MC Dropout predictions for the given input data. \n",
    "  # The shape of the output should be the same as the model's output layer.\n",
    "def mc_dropout_predict(model, X, n_samples=20):\n",
    "    # Write your code here\n",
    "    return output_mc\n",
    "\n",
    "# call mc_dropout_predict to Compute the MC Dropout predictions for model 2\n",
    "output_mc = ...\n",
    "\n",
    "# Compute the accuracy using MC Dropout\n",
    "accuracy_mc = ...\n",
    "\n",
    "# Display result.\n",
    "print(f\"Model 2 with MC Dropout: Test accuracy: {accuracy_mc:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step12_sol = (output_mc, accuracy_mc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a18e671-e0d9-4245-883f-ea97cda707fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91debe",
   "metadata": {},
   "source": [
    "## Learning Rate (LR) scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268228c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP13** Retrain model1 using performance scheduling and see if it improves training speed and model accuracy.\n",
    "\n",
    "**IMPORTANT:** Define the model the same way model1 was defined and compile the model the same way as model1.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb905e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Do not modify the following line of code.\n",
    "history1_perf = None\n",
    "\n",
    "# Redefine Model 1 so we start again with random weights\n",
    "model1_perfLRS = ...\n",
    "    ...\n",
    "\n",
    "early_stopping_cb = ...\n",
    "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
    "\n",
    "# Define the learning rate schedule\n",
    "lr_scheduler = ...\n",
    "\n",
    "# Creating model checkpoint to save the best model. \n",
    "dnn_bn_perf_checkpoint_cb = ...\n",
    "\n",
    "#####Perform the training on your machine and then comment out the following section before uploading it to gradescope.\n",
    "####################################  \n",
    "# Make sure your best model is named as follows:\n",
    "# Model1 with performance scheduling filename = best_dnn_bn_perf_model.keras\n",
    "\n",
    "\n",
    "# Train the model using early stopping and exponential scheduling (Comment this out before submission)\n",
    "history1_perf = ...\n",
    "\n",
    "# The following code will save your history - don't change it - comment it out before uploading to GS\n",
    "if \"history1_perf\" in globals():\n",
    "    with open('./history1_perf', 'wb') as file_pi:\n",
    "        pickle.dump(history1_perf.history, file_pi)\n",
    "####################################\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step13_sol = (dnn_bn_perf_checkpoint_cb, lr_scheduler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3d15e1-9397-44dd-b4b1-8b498f707550",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ebcef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP14** Using the loaded model obtained from the code given below, evaluate the performance of the model on the test set.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91b2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change the following 2 lines of code.\n",
    "# define the file name for the saved model\n",
    "model_name = \"best_dnn_bn_perf_model.keras\"\n",
    "# load the model\n",
    "model1_perf = keras.models.load_model(model_name)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss1_perf, test_acc1_perf = ...\n",
    "print(f\"Model 1 with performance scheduling: Test accuracy: {test_acc1_perf:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step14_sol = (test_loss1_perf, test_acc1_perf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "875be856-7c68-4495-933b-6e8f1e372a4d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d19f6f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**STEP15** From the history of the models loaded using the code given below, plot a graph of **accuracy** vs **number of epochs** for both training and validation.\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06b48c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load history for model 1 with learning rate scheduling (do not modify the following code)\n",
    "history_name1_perf = \"./history1_perf\"\n",
    "with open(history_name1_perf, \"rb\") as file_pi:\n",
    "    loaded_history1_perf = pickle.load(file_pi)\n",
    "\n",
    "# Load history for the original model1 (do not modify the following code)\n",
    "history_name1 = \"./history1\"\n",
    "with open(history_name1, \"rb\") as file_pi:\n",
    "    loaded_history1 = pickle.load(file_pi)\n",
    "\n",
    "# Plot the training and validation accuracy for both models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e06c852d-1da7-4d21-97b4-7f6cf7389f5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e2ee7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Use transfer learning by using a pre-trained **MobileNetV3Small** model on imagenet dataset, and fine-tuning it on the Sign Language MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb24bd9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP16** First of all, you to prepare your data for this model.\n",
    "* Reshape your input data (train and test) to (28, 28, 3).\n",
    "* Standardize your input. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbfc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape train data to (num_samples, 28, 28, 3)\n",
    "train_images_mb = ...\n",
    "\n",
    "# Reshape test data to (num_samples, 28, 28, 3)\n",
    "test_images_mb = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step16_sol = (train_images_mb,test_images_mb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6f14953-66d0-46e3-bb53-abfb2cb34327",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d86d3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step17** Now, we need to define and set up the model. For this you need to follow the next steps:\n",
    "\n",
    "* Load the pre-trained **MobileNetV3Small** model with weights from ImageNet.\n",
    "* Modify the model to accept inputs of shape (56, 56, 3). \n",
    "* Freeze the weights of the pretrained layers.* \n",
    "* Add input layer that take an image of shape (28,28,3), then a layer `UpSampling2D` to upscale the input by a factor of 2 so they can go through the network.\n",
    "\n",
    "Consider that maybe you need to adapt the default output.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414e273-4f69-42e5-9523-73d224f9d126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "...\n",
    "from tensorflow.keras.applications import MobileNetV3Small \n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "# Load the pre-trained MobileNet model with weights from ImageNet\n",
    "base_mb_model = ...\n",
    "\n",
    "\n",
    "# Create the new model\n",
    "final_mb_model = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step17_sol = final_mb_model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f93da8d6-168d-4996-be7f-8e7e8118a3f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93db0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step18** Once we define the model and do the specific modifications to adjust to our data, we compile it.\n",
    "\n",
    "* Use a learning rate schedule that uses an exponential decay schedule.\n",
    "* Compile and train the model.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d73560-3662-4f21-ba69-bc23a02320b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an initial learning rate                            \n",
    "initial_learning_rate = ...\n",
    "\n",
    "# Create the proper learning rate schedule\n",
    "lr_schedule = ...\n",
    "\n",
    "# Compile model\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step18_sol = (lr_schedule, final_mb_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afe4ae05-bf5e-4374-975f-5674e2cfa7f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761f99b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step19** Train the mobilenet model. Include early stopping in your training procedure.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f75ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
    "# Do not modify the following line of code.\n",
    "history_mb = None\n",
    "\n",
    "# Define model checkpoint callback (Do not modify this code).\n",
    "best_model_checkpoint = ...\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping_cb = ...\n",
    "\n",
    "## Perform the training on your machine and then comment out the following section before uploading it to gradescope. \n",
    "#################################### \n",
    "# make sure your best model is named as follow:\n",
    "# MobileNet model filename = best_mobilenet_model.keras\n",
    "# Train the model (comment this section out)\n",
    "history_mb = ...\n",
    "\n",
    "# The following code will save your history - don't change it\n",
    "if \"history_mb\" in globals():\n",
    "    with open('./historymb', 'wb') as file_pi:\n",
    "        pickle.dump(history_mb.history, file_pi)    \n",
    "####################################\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step19_sol = (best_model_checkpoint, early_stopping_cb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69a3bfa9-6c67-4ec9-8e23-6c752719ee25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f5bf8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step20** For the trained model loaded using the code given below, evaluate its performance on the test set.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c256e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not modify the following two lines of code.\n",
    "# define the file name for the saved model\n",
    "model_name = \"best_mobilenet_model.keras\"\n",
    "# load the model\n",
    "final_mb_model = keras.models.load_model(model_name)\n",
    "\n",
    "# Reshape the input data to (num_samples, 28, 28, 3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss1_mobilenet, test_acc1_mobilenet = ...\n",
    "print(f\"Model Mobile Net: Test accuracy: {test_acc1_mobilenet:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step20_sol = (test_loss1_mobilenet,test_acc1_mobilenet)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20e1c8e6-3a4b-466a-9af4-944e91c49408",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c3477",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step21** So far, you have seen the overall performance of your models. However, it is possible that some classes may be more difficult to classify than others. To gain a clearer understanding of which letters are the most difficult or easiest to predict, you can use your MobileNet model and make predictions on your test data using the predict function. Based on this, you can check the proportion of correct matches for each letter over the total number of that specific letter in the test data (as the proportion of one letter may differ from that of others). Finally, return the result as a string indicating the most complex and easiest letter to predict based on our analysis (e.g., \"a\" in lowercase).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd430f-6e53-4b30-bdd4-99efe8a35a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put here again the labels (not hot encoded)\n",
    "test_labels = ...\n",
    "\n",
    "# Make the prediction using MobileNet model. Use the function predict.\n",
    "prediction_test = ...\n",
    "\n",
    "# What is the most difficult letter to predict? (if you have many letters which are equally difficult to predict, pick up any of them. Only one and put in a string (e.g. \"a\"))\n",
    "complex_letter = ...\n",
    "\n",
    "# What is the easist letter to predict? (if you have many letters which are equally easy to predict, pick up any of them. Only one and put in a string (e.g. \"a\"))\n",
    "easiest_letter = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step21_sol = (test_labels,prediction_test,complex_letter,easiest_letter)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c65524ff-426a-4641-8034-d5af4b0debfe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e0a43",
   "metadata": {},
   "source": [
    "## Using our final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a7fea",
   "metadata": {},
   "source": [
    "Finally, so far you got a powerful model capable to use it to predict in new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd3c26",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step22** \n",
    "\n",
    "**Predict on a new sample** Process the image `challenge1.jpg` and try to dechiper what is the letter in the image using your best model. Be aware that your model gives you numeric results, however you should convert this result in a proper output of letters (use lowercase letters).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da4da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the image (do not modify this line of code)\n",
    "img_challenge1 = cv2.imread('challenge1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Plot the image (do not modify the following 2 lines of code)\n",
    "plt.imshow(img_challenge1)\n",
    "plt.show()\n",
    "\n",
    "# Process the data\n",
    "\n",
    "# Predict in this data using your best model\n",
    "prediction_challenge1 = ...\n",
    "\n",
    "\n",
    "# Decoding result. This should be the string representation of the output generated by your model.\n",
    "result_challenge1 = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step22_sol = (result_challenge1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b5906a5-bc74-4918-8cdd-fb64f904c2e6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step22\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
