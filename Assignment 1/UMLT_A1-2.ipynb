{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe98470-f20c-4017-8970-0fc425f0ca4b",
   "metadata": {},
   "source": [
    "# Using Machine Learning Tools Assignment 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will apply some popular machine learning techniques to the problem of predicting bike rental demand. A data set has been provided containing records of bike rentals in Seoul, collected during 2017-18.\n",
    "\n",
    "## General instructions\n",
    "\n",
    "This assignment is divided into several tasks. Use the spaces provided in this notebook to answer the questions posed in each task. Note that some questions require writing a small amount of code and some require graphical results. It is your responsibility to make sure your responses are clearly labelled and your code has been fully executed (with the correct results displayed) before submission!\n",
    "\n",
    "**Do not** manually edit the data set file we have provided! For marking purposes, it's important that your code is written to run correctly on the original data file.\n",
    "\n",
    "When creating graphical output, label is clearly, with appropriate titles, xlabels and ylabels, as appropriate.\n",
    "\n",
    "Chapter 2 of the textbook is based on a similar workflow to this assignment, so you may look there for some further background and ideas. You can also use any other general resources on the internet that are relevant although do not use ones which directly relate to these questions with this dataset (which would normally only be found in someone else's assignment answers). If you take a large portion of code or text from the internet then you should reference where this was taken from, but we do not expect any references for small pieces of code. Taking, and adapting, small portions of code is expected and is common practice when solving real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a08e03-e275-4ab1-80bd-8128e15a58c7",
   "metadata": {},
   "source": [
    "## The following code imports some of the essential libraries that you will need. You should not need to modify it, but you are expected to import other libraries as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c262c3-9ded-4f98-aafa-62b3d8c63d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67336-c1a7-4592-afca-381db3d39261",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP01:** \n",
    "Load the data set from the csv file (SeoulBikeData.csv) into a DataFrame, and summarise it with the pandas functions `describe()` and `info()`.\n",
    "\n",
    "Download the data set from MyUni using the link provided on the assignment page. A paper that describes one related version of this dataset is: Sathishkumar V E, Jangwoo Park, and Yongyun Cho. 'Using data mining techniques for bike sharing demand prediction in metropolitan city.' Computer Communications, Vol.153, pp.353-366, March, 2020. Feel free to look at this if you want more information about the dataset.\n",
    "\n",
    "The data is stored in a CSV (comma separated variable) file and contains the following information \n",
    "\n",
    " - Date: year-month-day\n",
    " - Rented Bike Count: Count of bikes rented at each hour\n",
    " - Hour: Hour of the day\n",
    " - Temperature: Temperature in Celsius\n",
    " - Humidity: %\n",
    " - Windspeed: m/s\n",
    " - Visibility: 10m\n",
    " - Dew point temperature: Celsius\n",
    " - Solar radiation: MJ/m2\n",
    " - Rainfall: mm\n",
    " - Snowfall: cm\n",
    " - Seasons: Winter, Spring, Summer, Autumn\n",
    " - Holiday: Holiday/No holiday\n",
    " - Functional Day: NoFunc(Non Functional Hours), Fun(Functional hours)\n",
    "\n",
    "**Load the data set from the csv file into a DataFrame, and summarise it with at least two appropriate pandas functions.**\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b7803-61e3-4181-b20b-714c0a1ec542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SeoulBikeData.csv\")\n",
    "\n",
    "data.info()\n",
    "data.describe()\n",
    "data.head()\n",
    "\n",
    "# The following code is used by the autograder, don't change it\n",
    "step1_sol = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67aa65-c02d-417f-8977-81edc91fea7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff81dd-e750-4ee7-96c0-65d0f9152b1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP02:** To get a feeling for the data it is a good idea to do some form of simple visualisation. Display a set of histograms for the features as they are right now, prior to any cleaning steps.\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c030f-3cfe-4c56-86a3-1b6b0d52d4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create 4x3 subplots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 12))  \n",
    "\n",
    "# Plot histograms for all columns \n",
    "for i, col in enumerate(data.columns):\n",
    "    sns.histplot(data[col], ax=axes[i//4, i%4], kde=True)\n",
    "    axes[i//4, i%4].set_title(col)\n",
    "    axes[i//4, i%4].set_xlabel('')\n",
    "\n",
    "fig.suptitle('Distribution of Bike Renting Factors', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69b11b-e5b9-4fe7-9609-6bc49b54fdfd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f972b-279b-4c48-b088-e342a264bce3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP03:** The \"Functioning Day\" feature records whether the bike rental was open for business on that day. For this assignment we are only interested in predicting demand on days when the business is open, so remove rows from the DataFrame where the business is closed. After doing this, delete the Functioning Day feature from the DataFrame and verify that this worked. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13def6-6cd2-4350-90e7-d19d857b680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(data[data['Functioning Day'] != \"Yes\" ].index)\n",
    "data = data.drop('Functioning Day', axis=1)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step3_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44529a2b-6879-4967-bce5-9a9e1fb7e1fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba264cb-6af0-428c-9dd6-e9a3ee677f8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP04:** \n",
    "The goal is to predict bike rental demand using historical data. To achieve this, you will use regression techniques with \"Bike Rental Count\" as the target feature for this prediction, *but for this*, it is important that all other features in the data are numerical. Two of the features in the data, \"Holiday\" and \"Season\", need to be converted to numerical format. Write code to convert the \"Holiday\" feature to 0 or 1 from its current format. For the \"Season\" feature, add 4 new columns, labeled as \"Winter\", \"Spring\", \"Summer\", and \"Autumn\" then remove the \"Season\" column. Each of these columns should store a 0 or 1, depending on the corresponding season in each row. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81b7dd-84ba-4eea-ba70-b676bd9fa185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Map Holiday state to 1 and No Holiday to 0\n",
    "data['Holiday'] = data['Holiday'].map({'Holiday': 1, 'No Holiday': 0})\n",
    "\n",
    "# Convert elements in Seasons to dummies variable\n",
    "seasons = pd.get_dummies(data[\"Seasons\"], prefix=None, drop_first=False, dtype=int)  \n",
    "data.drop(\"Seasons\", axis=1, inplace=True)  \n",
    "data = pd.concat([data, seasons], axis=1)\n",
    "\n",
    "\n",
    "# The following code is used by the autograder, don't change it\n",
    "step4_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49402cfb-2331-4524-81ad-a750e8690edf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621bb16-d096-49b7-a73b-c4ecbd253f6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP05**: It is known that bike rentals depend strongly on whether it's a weekday or a weekend. Replace the Date feature with a Weekday feature that stores 0 or 1 depending on whether the date represents a weekend or weekday. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69895a7f-3694-40a1-acf0-36a48f4a5baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert from object to pandas date format\n",
    "data[\"Date\"]= pd.to_datetime(data[\"Date\"],format= \"%d/%m/%Y\")\n",
    "\n",
    "# find day of the week from the date\n",
    "data[\"Date\"] = data[\"Date\"].dt.day_of_week\n",
    "\n",
    "# define condition for weekday\n",
    "def weekday_def(day):\n",
    "    if day >= 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# map conditions across columns\n",
    "data[\"Date\"] = data[\"Date\"].map(weekday_def)\n",
    "\n",
    "# rename columns\n",
    "data.rename(columns={\"Date\":\"Weekday\"}, inplace=True)\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step5_data\n",
    "step5_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354885a-254d-4790-a1db-0ce9bf7fccdf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3b55b-53b8-4293-a6a0-42c4b478ed0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP06** Convert all the remaining data to numerical format, with any non-numerical entries set to NaN.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f87537-10a4-4216-9cc9-862acfe5e5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert rainfall and snowfall to numeric\n",
    "data[\"Rainfall(mm)\"] = pd.to_numeric(data[\"Rainfall(mm)\"], errors='coerce')\n",
    "data[\"Snowfall (cm)\"] = pd.to_numeric(data[\"Snowfall (cm)\"], errors='coerce')\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that DataFrame from this step is the one assigned to step6_data\n",
    "step6_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55748aaa-00d5-4475-96ae-b8b48188dcbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17368290-e030-4f1f-9e25-a71386952da8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP07** Examin the data and identify problematic entries. Set any problematic values in the numerical data to `np.nan` and check that this has worked. Once this is done, specify a **sklearn *pipeline* that will perform imputation** to replace problematic entries (nan values) with an appropriate **median** value ***and* any other pre-processing** that you think should be used. Just specify the pipeline - do ***not*** run it now.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32f339-8d1e-4351-8b7d-62a5f1a4b11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set negative values from Humidity and Visibility to nan\n",
    "data[\"Humidity (%)\"] = data[\"Humidity (%)\"].apply(lambda x: np.nan if x < 0 else x)\n",
    "data[\"Visibility (10m)\"] = data[\"Visibility (10m)\"].apply(lambda x: np.nan if x < 0 else x)\n",
    "\n",
    "# keep the variable name pipeline_step7 as you will use it in STEP09\n",
    "pipeline_step7 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "    ('scaler', StandardScaler())  # Standardize features\n",
    "])\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains the solutions from this step is the one assigned to step7_data as follows\n",
    "step7_data = [data.copy(),pipeline_step7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660fe6c-fa1a-4ceb-bba3-d8b8a40d79a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c1079-68f5-4881-86b2-f45dd92ca557",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP08:** Generate a pre-processed version of the entire dataset by applying the pipeline defined in STEP07. Then, calculate the correlation of each feature with the target using either the pandas function corr() or numpy corrcoef() and find the 3 attributes that are the most correlated with bike rentals. \n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933518f8-fd5d-4df4-9f69-74b46a15aa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data = pipeline_step7.fit_transform(data)\n",
    "\n",
    "correlation = data.corr()\n",
    "\n",
    "# correlation heat map\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.show()\n",
    "\n",
    "# top_3 should be an array of 3 strings ['attribute name', 'attribute name','attribute name']\n",
    "top_3 = ['Temperature (C)', 'Winter', 'Hour']\n",
    "print(top_3)\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains a list of the names of the top 3 attributes is assigned to step3_data\n",
    "step8_data = top_3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf23bf-5c11-4cb9-acc2-670fd7c7ef04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930044f-1a46-4cdd-bc31-11cf9ba86c40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP09:** Divide the data into training and test sets using where 20% of the data is kept for testing. Create a pipeline that includes the linear regression model in addition to the pipeline defined in STEP07. Fit the pipeline to the training set and calculate the `rmse` of the fit to evaluate its performance. As a comparison, compute the `rmse` that would be obtained by predicting the mean value of bike rentals for all training examples.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7b511-5127-44fc-95e0-2adf4e3ac2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step9\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pipeline_step9 = make_pipeline(pipeline_step7, LinearRegression())\n",
    "\n",
    "y_train, y_test = train_test_split(data['Rented Bike Count'], test_size=0.2, random_state=42)\n",
    "X_train, x_test = train_test_split(data.drop('Rented Bike Count', axis=1), test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# calculate the RMSE of the fit to the training data\n",
    "result = pipeline_step9.fit(X_train, y_train)\n",
    "rmse_train = mean_squared_error(y_train, result.predict(X_train), squared=False)\n",
    "# calculate the RMSE of the baseline model (by predicting the mean value of bike rentals for all training examples)\n",
    "baseline_pred = y_train.mean()\n",
    "rmse_baseline = mean_squared_error(y_train, [baseline_pred]*len(y_train), squared=False)\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"RMSE for baseline (predicting mean):\", rmse_baseline)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step9_data = [rmse_train,rmse_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f419c24-d2b5-4c36-90fa-ad9f64b53a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The following code will show a visualisation of the fit for your linear regression.\n",
    "# I will use your pipeline_step9 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step9.predict(X_train[:subset_size])\n",
    "\n",
    "# Then I create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "# A perfect solution would look like the red line\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bd3ed-0f95-49a1-af42-2140c2f5c71b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b942b-85cd-41ff-9c2f-91f4e0b4b4af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP10:**  Fit a Kernel Ridge regression model (imported from sklearn.kernel_ridge) to the X_train data from STEP09. Build a new pipeline that includes the Kernel Ridge regression model in addition to the pipeline defined in STEP07, and fit it to the training data using default settings. Generate a scatter plot of the predicted values against the actual values for the training data, and calculate the RMSE of the fit to the training data.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5180a-f6b0-466d-9ce7-b81a9b64f465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step10\n",
    "pipeline_step10 = make_pipeline(pipeline_step7, KernelRidge())\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_KR = pipeline_step10.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_KR = mean_squared_error(y_train, y_pred_train_KR, squared=False)\n",
    "print('Kernel Ridge model RMSE on training data:', rmse_train_KR)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step10_data = [rmse_train_KR,pipeline_step10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dd3ae-dea6-4555-8d49-eb2dba3b82ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I will use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step10.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8bdab-ad38-418a-84df-d5ebd183c178",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98a96c-61a0-40b2-bb88-6eb1b431546c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP11:** Fit a Support Vector Regression (from sklearn.svm import SVR). As you did for STEP10, create a new pipeline using the pipelinr from STEP07 and this model and fit it to your training data, using the default settings. Again, calculate the RMSE of the fit to the training data.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3fcdb-18a2-42f2-ab54-b691a811a7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step11\n",
    "pipeline_step11 = make_pipeline(pipeline_step7, SVR())\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_SVR = pipeline_step11.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_SVR = mean_squared_error(y_train, y_pred_train_SVR, squared=False)\n",
    "print('Support Vector Regression model RMSE on training data:', rmse_train_SVR)\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step11_data = [rmse_train_SVR,pipeline_step11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019c7c4-2e67-4111-9d1c-a136ae45d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step11.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4005b2-6645-4541-ab27-2b5cb27f531e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1323f6-6969-47ef-a614-419b4722cabb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP12:** Perform a 10 fold cross validation for each of the three model (LinearRegression,KernelRidge,SVR). This splits the training set (that we've used above) into 10 equal size subsets, and uses each in turn as the validation set while training a model with the other 9. You should therefore have 10 rmse values for each cross validation run. Find the mean and standard deviation of the rmse values obtained for each model for the validation splits.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57801463-c15f-493c-bb4b-32670168b0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you might need some or all of the following imports\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#Linear Regression CV mean and std RMSE from the 10 folds:\n",
    "rmse_LR = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train, X_test = data.drop('Rented Bike Count', axis=1).iloc[train_index], data.drop('Rented Bike Count', axis=1).iloc[test_index]\n",
    "    y_train, y_test = data['Rented Bike Count'].iloc[train_index], data['Rented Bike Count'].iloc[test_index]\n",
    "    result = pipeline_step9.fit(X_train, y_train)\n",
    "    rmse = mean_squared_error(y_test, result.predict(X_test), squared=False)\n",
    "    rmse_LR.append(rmse)\n",
    "    \n",
    "rmse_LR_mean = np.mean(rmse_LR)\n",
    "rmse_LR_std  = np.std(rmse_LR)\n",
    "print('Linear Regression CV Scores:') \n",
    "print(f'Mean: {rmse_LR_mean:.2f}, Std: {rmse_LR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "# Kernel Ridge Regression CV mean and std:\n",
    "rmse_KR = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train, X_test = data.drop('Rented Bike Count', axis=1).iloc[train_index], data.drop('Rented Bike Count', axis=1).iloc[test_index]\n",
    "    y_train, y_test = data['Rented Bike Count'].iloc[train_index], data['Rented Bike Count'].iloc[test_index]\n",
    "    result = pipeline_step10.fit(X_train, y_train)\n",
    "    rmse = mean_squared_error(y_test, result.predict(X_test), squared=False)\n",
    "    rmse_KR.append(rmse)\n",
    "rmse_KR_mean = np.mean(rmse_KR)\n",
    "rmse_KR_std  = np.std(rmse_KR)\n",
    "print('Kernel Ridge Regression CV Scores:') \n",
    "print(f'Mean: {rmse_KR_mean:.2f}, Std: {rmse_KR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "# Support Vector Regression CV mean and std:\n",
    "rmse_SVR = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train, X_test = data.drop('Rented Bike Count', axis=1).iloc[train_index], data.drop('Rented Bike Count', axis=1).iloc[test_index]\n",
    "    y_train, y_test = data['Rented Bike Count'].iloc[train_index], data['Rented Bike Count'].iloc[test_index]\n",
    "    result = pipeline_step11.fit(X_train, y_train)\n",
    "    rmse = mean_squared_error(y_test, result.predict(X_test), squared=False)\n",
    "    rmse_SVR.append(rmse)\n",
    "rmse_SVR_mean = np.mean(rmse_SVR)\n",
    "rmse_SVR_std  = np.std(rmse_SVR)\n",
    "print('Support Vector Regression CV Scores:') \n",
    "print(f'Mean: {rmse_SVR_mean:.2f}, Std: {rmse_SVR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step12_data = [rmse_LR_mean,rmse_KR_mean,rmse_SVR_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac329bf-9f88-450b-98f2-f0982e38777e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83026fa4-6a39-493e-adf8-db14904cc024",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP13:** Both the Kernel Ridge Regression and Support Vector Regression have hyperparameters that can be adjusted to suit the problem. Use grid search to systematically compare the generalisation performance (rmse) obtained with different hyperparameter settings (still with 10-fold CV). Use the sklearn function GridSearchCV to do this.\n",
    "\n",
    "For KernelRidge, vary the hyperparameter alpha. (note, if you are using KernelRidge as the last step in a pipeline, alpha is refered to as kernelridge__alpha) \n",
    "\n",
    "For SVR, vary the hyperparameter C. (note, if you are using SVR as the last step in a pipeline, C is refered to as SVR__C)\n",
    "\n",
    "Find the hyperparameter setting for each medel.\n",
    "\n",
    "Finally, train and apply both models, with the best hyperparameter settings, to the test set and report the performance as rmse.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcc456cd-be46-4a94-8d79-8c015560279e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m svr_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline_step11, param_grid\u001b[38;5;241m=\u001b[39msvr_param, cv\u001b[38;5;241m=\u001b[39mkfold, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV objects to the training data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mkr_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m svr_cv\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameter setting for each model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/kernel_ridge.py:210\u001b[0m, in \u001b[0;36mKernelRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m     ravel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    209\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_ \u001b[38;5;241m=\u001b[39m \u001b[43m_solve_cholesky_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ravel:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:237\u001b[0m, in \u001b[0;36m_solve_cholesky_kernel\u001b[0;34m(K, y, alpha, sample_weight, copy)\u001b[0m\n\u001b[1;32m    231\u001b[0m K\u001b[38;5;241m.\u001b[39mflat[:: n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Note: we must use overwrite_a=False in order to be able to\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m#       use the fall-back solution below in case a LinAlgError\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m#       is raised\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     dual_coef \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m    239\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix in solving dual problem. Using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleast-squares solution instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scipy/linalg/_basic.py:213\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[1;32m    210\u001b[0m     trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    211\u001b[0m     norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 213\u001b[0m anorm \u001b[38;5;241m=\u001b[39m \u001b[43mlange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Generalized case 'gesv'\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assume_a \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('Rented Bike Count', axis=1), data['Rented Bike Count'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the GridSearchCV objects for each model\n",
    "kr_param = {'kernelridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100,1000]}\n",
    "kr_cv = GridSearchCV(pipeline_step10, param_grid=kr_param, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "svr_param = {'svr__C': [0.001, 0.01, 0.1, 1, 10, 100,1000]}\n",
    "svr_cv = GridSearchCV(pipeline_step11, param_grid=svr_param, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV objects to the training data\n",
    "kr_cv.fit(X_train, y_train)\n",
    "svr_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameter setting for each model\n",
    "print(\"Best hyperparameter setting for Kernel Ridge Regression:\", kr_cv.best_params_)\n",
    "print(\"Best hyperparameter setting for Support Vector Regression:\", svr_cv.best_params_)\n",
    "\n",
    "# Create pipeline using the best hyperparameter\n",
    "pipeline_best_kr = kr_cv.best_estimator_\n",
    "pipeline_best_svr = svr_cv.best_estimator_\n",
    "\n",
    "# Train and apply the chosen model to the test set\n",
    "kr_predictions = pipeline_best_kr.fit(X_train, y_train).predict(x_test)\n",
    "kr_rmse = mean_squared_error(y_test, kr_predictions, squared=False)\n",
    "\n",
    "svr_predictions = pipeline_best_svr.fit(X_train, y_train).predict(x_test)\n",
    "svr_rmse = mean_squared_error(y_test, svr_predictions, squared=False)\n",
    "\n",
    "print(\"Kernel Ridge Regression RMSE on test set:\", kr_rmse)\n",
    "print(\"Support Vector Regression RMSE on test set:\", svr_rmse)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step13_data = [kr_rmse , svr_rmse, kr_cv , svr_cv,pipeline_best_kr,pipeline_best_svr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bbb82-c4bf-491e-94db-24fdb26c405d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_best_svr to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_best_svr.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b7d78-6f91-4e88-8110-f46389bd9b83",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250a081-ee86-43a2-a8ff-a744004e6af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "step01": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step01\"\npoints = 2\n\n@test_case(points=2, hidden=False, \n    failure_message=\"data shape is incorrect\")\ndef test_q1(step1_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(step1_sol,(8760, 14),\"error\")\n",
    "step02": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step02\"\npoints = 0\n\n@test_case(points=0, hidden=False)\ndef test_q2():\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(1,1)\n",
    "step03": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step03\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes by removing the rows and the column based on the question specification?\")\ndef test_q3(step3_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(step3_data.loc[step3_data['Rented Bike Count'] == 0].shape[0],0)\n    TC.assertNotIn('Functioning Day', step3_data.columns)\n",
    "step04": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step04\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes by changing the rows and removing the column based on the question specification?\")\ndef test_q4(step4_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step4_data\n    TC.assertIn('Holiday', D.columns)\n    TC.assertTrue((D['Holiday'] == 0).any())\n    TC.assertTrue((D['Holiday'] == 1).any())\n\n    # check that the 'Season' feature has been converted to 4 new columns\n    TC.assertIn('Winter', D.columns)\n    TC.assertIn('Spring', D.columns)\n    TC.assertIn('Summer', D.columns)\n    TC.assertIn('Autumn', D.columns)\n    TC.assertNotIn('Seasons', D.columns)\n",
    "step05": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step05\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes on the question specification?\")\ndef test_q5(step5_data):\n    import pandas as pd\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step5_data\n    TC.assertTrue(pd.api.types.is_numeric_dtype(D['Weekday']))\n    TC.assertIn('Weekday', D.columns)\n    TC.assertNotIn('Date', D.columns)\n",
    "step06": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step06\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes based on the question specification?\")\ndef test_q6(step6_data):\n    import pandas as pd\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step6_data\n    for col in D.columns:\n        TC.assertTrue(pd.api.types.is_numeric_dtype(D[col]))\n",
    "step07": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step07\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Your pipeline is not correct or there is still problematic entries in your data\")\ndef test_q7(step7_data):\n    from sklearn.pipeline import Pipeline\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step7_data[0]\n    pipeline_step7 = step7_data[1]\n    TC.assertIsInstance(pipeline_step7, Pipeline)\n    #TC.assertIsInstance(pipeline_step7.named_steps['imputer'], SimpleImputer)\n    TC.assertTrue(D['Humidity (%)'].isna().sum() > 0)\n",
    "step08": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step08\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Your top3 list is not correct\")\ndef test_q8(step8_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertListEqual(sorted(list(step8_data)),sorted(['Hour', 'Temperature (C)', 'Winter']))\n",
    "step09": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step09\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"rmse for the baseline is better than your trained model\")\ndef test_q9(step9_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rmse_train    = step9_data[0]\n    rmse_baseline = step9_data[1]\n    TC.assertTrue(rmse_train < rmse_baseline)\n    #TC.assertAlmostEqual(len(X_test)/(len(X_train) + len(X_test)),0.2)\n",
    "step10": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step10\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"the RMSE of your model is too high or your pipeline is not correct\")\ndef test_q10(step10_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rms = step10_data[0]\n    pipeline = step10_data[1]\n    TC.assertTrue(rms < 1000)\n    TC.assertTrue('KernelRidge' in str(pipeline.named_steps))\n",
    "step11": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step11\"\npoints = 4\n\n@test_case(points=4, hidden=False, \n    failure_message=\"the RMSE of your model is too high or your pipeline is not correct\")\ndef test_q11(step11_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rms = step11_data[0]\n    pipeline = step11_data[1]\n    TC.assertTrue(rms < 800)\n    TC.assertTrue('SVR' in str(pipeline.named_steps))\n",
    "step12": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step12\"\npoints = 5\n\n@test_case(points=5, hidden=False, \n    failure_message=\"one or more RMSE of your models are too high or your pipeline is not correct\")\ndef test_q12(step12_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(step12_data[0] < 1000)\n    TC.assertTrue(step12_data[1] < 1000)\n    TC.assertTrue(step12_data[2] < 1000)\n",
    "step13": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step13\"\npoints = 5\n\n@test_case(points=5, hidden=False, \n    failure_message=\"one or more RMSE of your models are too high or your pipeline is not correct\")\ndef test_q13(step13_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(step13_data[0] < 900)\n    TC.assertTrue(step13_data[1] < 300)\n    # Check grid search setup and results for KernelRidge\n    TC.assertIn('kernelridge__alpha', step13_data[2].param_grid)\n    TC.assertTrue(hasattr(step13_data[2], 'best_params_'))\n    TC.assertIn('kernelridge__alpha', step13_data[2].best_params_)\n    \n    # Check grid search setup and results for SVR\n    TC.assertIn('svr__C', step13_data[3].param_grid)\n    TC.assertTrue(hasattr(step13_data[3], 'best_params_'))\n    TC.assertIn('svr__C', step13_data[3].best_params_)\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
